{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maria-lin/How-to-make-notebook-in-dataiku/blob/main/Detection_Anomalies_DAB_Professionnel_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFxcZ_kjYr2c"
      },
      "source": [
        "# üè¶ D√©tection d'Anomalies sur les Distributeurs Automatiques (DAB)\n",
        "## Analyse Senior - Approche Multi-Algorithmes\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Objectifs\n",
        "- D√©tecter comportements anormaux des DAB (fraude, dysfonctionnement, usage atypique)\n",
        "- Approche non-supervis√©e avec validation crois√©e multi-algorithmes\n",
        "- Interpr√©tabilit√© et recommandations actionnables\n",
        "\n",
        "### üéØ M√©thodologie\n",
        "1. Audit qualit√© et EDA approfondie\n",
        "2. Feature engineering m√©tier\n",
        "3. D√©tection multi-algorithmes (IF + LOF + One-Class SVM)\n",
        "4. Consensus et scoring\n",
        "5. Interpr√©tabilit√© et validation\n",
        "6. Export et recommandations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgM4pVcfYr2e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS ET CONFIGURATION\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# ML & Stats\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import stats\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"‚úÖ ENVIRONNEMENT CONFIGUR√â\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üìÖ Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üé≤ Random State : {RANDOM_STATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65rwfoqYYr2g"
      },
      "source": [
        "---\n",
        "# üì• SECTION 1 : Chargement des Donn√©es\n",
        "\n",
        "‚ö†Ô∏è **ACTION REQUISE** : Modifiez `DATASET_NAME` avec le nom de votre dataset Dataiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1spgfWYr2g"
      },
      "outputs": [],
      "source": [
        "import dataiku\n",
        "\n",
        "# üîß PARAM√àTRE √Ä CONFIGURER\n",
        "DATASET_NAME = \"VOTRE_DATASET_DAB\"  # ‚ö†Ô∏è MODIFIER ICI\n",
        "\n",
        "try:\n",
        "    dataset = dataiku.Dataset(DATASET_NAME)\n",
        "    df = dataset.get_dataframe()\n",
        "\n",
        "    print(f\"‚úÖ Dataset charg√© : {DATASET_NAME}\")\n",
        "    print(f\"üìä Dimensions : {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes\")\n",
        "    print(f\"üíæ M√©moire : {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
        "\n",
        "    display(df.head(10))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur : {e}\")\n",
        "    print(\"\\nüí° V√©rifiez :\")\n",
        "    print(\"   1. Le nom du dataset est correct\")\n",
        "    print(\"   2. Vous avez les permissions\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vUhKe-Yr2g"
      },
      "source": [
        "---\n",
        "# üîç SECTION 2 : Audit Qualit√© Complet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNIoFAl3Yr2g"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"üìä AUDIT QUALIT√â\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 2.1 Structure\n",
        "print(\"\\nüîé Informations g√©n√©rales :\")\n",
        "df.info()\n",
        "\n",
        "# 2.2 Valeurs manquantes\n",
        "print(\"\\nüï≥Ô∏è Valeurs manquantes :\")\n",
        "missing = pd.DataFrame({\n",
        "    'nb_missing': df.isnull().sum(),\n",
        "    'pct_missing': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "}).sort_values('pct_missing', ascending=False)\n",
        "missing = missing[missing['nb_missing'] > 0]\n",
        "\n",
        "if len(missing) > 0:\n",
        "    display(missing)\n",
        "else:\n",
        "    print(\"‚úÖ Aucune valeur manquante\")\n",
        "\n",
        "# 2.3 Doublons\n",
        "dup_rows = df.duplicated().sum()\n",
        "print(f\"\\nüîÑ Doublons de lignes : {dup_rows:,}\")\n",
        "if 'num_automate' in df.columns:\n",
        "    dup_id = df.duplicated(subset=['num_automate']).sum()\n",
        "    print(f\"üîÑ Doublons num_automate : {dup_id:,}\")\n",
        "\n",
        "# 2.4 Incoh√©rences\n",
        "print(\"\\n‚öôÔ∏è Contr√¥les de coh√©rence :\")\n",
        "checks = {}\n",
        "\n",
        "if 'montant_total' in df.columns:\n",
        "    checks['montant_negatif'] = (df['montant_total'] < 0).sum()\n",
        "    checks['montant_zero'] = (df['montant_total'] == 0).sum()\n",
        "\n",
        "if 'nb_total_de_retraits' in df.columns:\n",
        "    checks['nb_negatif'] = (df['nb_total_de_retraits'] < 0).sum()\n",
        "    checks['nb_zero'] = (df['nb_total_de_retraits'] == 0).sum()\n",
        "\n",
        "if 'montant_total' in df.columns and 'nb_total_de_retraits' in df.columns:\n",
        "    checks['montant_pos_nb_zero'] = ((df['montant_total'] > 0) & (df['nb_total_de_retraits'] == 0)).sum()\n",
        "    checks['montant_zero_nb_pos'] = ((df['montant_total'] == 0) & (df['nb_total_de_retraits'] > 0)).sum()\n",
        "\n",
        "checks_df = pd.DataFrame.from_dict(checks, orient='index', columns=['Nombre'])\n",
        "checks_df['Pourcentage'] = (checks_df['Nombre'] / len(df) * 100).round(2)\n",
        "display(checks_df.sort_values('Nombre', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B6cvpFMYr2g"
      },
      "source": [
        "---\n",
        "# üìä SECTION 3 : EDA - Analyse Exploratoire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhS0kBslYr2h"
      },
      "outputs": [],
      "source": [
        "# 3.1 Variables cat√©gorielles\n",
        "cat_cols = [c for c in ['lib_site_implementation','type_carte','dab_hos_site','typ_gab'] if c in df.columns]\n",
        "\n",
        "if cat_cols:\n",
        "    print(\"üè∑Ô∏è Variables cat√©gorielles :\\n\")\n",
        "    for col in cat_cols:\n",
        "        print(f\"\\n{col} - {df[col].nunique()} modalit√©s\")\n",
        "        display(df[col].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkN60a-UYr2h"
      },
      "outputs": [],
      "source": [
        "# 3.2 Variables num√©riques\n",
        "num_cols = [c for c in ['montant_total','nb_total_de_retraits'] if c in df.columns]\n",
        "\n",
        "if num_cols:\n",
        "    print(\"üî¢ Statistiques num√©riques :\\n\")\n",
        "    stats_df = df[num_cols].describe(percentiles=[.01,.05,.25,.5,.75,.95,.99]).T\n",
        "    stats_df['skewness'] = df[num_cols].skew()\n",
        "    stats_df['kurtosis'] = df[num_cols].kurtosis()\n",
        "    display(stats_df)\n",
        "\n",
        "    # Visualisations\n",
        "    fig, axes = plt.subplots(len(num_cols), 3, figsize=(15, 5*len(num_cols)))\n",
        "    if len(num_cols) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, col in enumerate(num_cols):\n",
        "        data = df[col].dropna()\n",
        "\n",
        "        # Histogramme\n",
        "        axes[idx,0].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
        "        axes[idx,0].axvline(data.median(), color='red', linestyle='--', label=f'M√©diane: {data.median():,.0f}')\n",
        "        axes[idx,0].set_title(f'Distribution - {col}')\n",
        "        axes[idx,0].legend()\n",
        "\n",
        "        # Boxplot\n",
        "        axes[idx,1].boxplot(data, vert=True)\n",
        "        axes[idx,1].set_title(f'Boxplot - {col}')\n",
        "\n",
        "        # Q-Q plot\n",
        "        stats.probplot(data, dist=\"norm\", plot=axes[idx,2])\n",
        "        axes[idx,2].set_title(f'Q-Q Plot - {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ5sVoEIYr2h"
      },
      "outputs": [],
      "source": [
        "# 3.3 Relation montant vs nombre de retraits\n",
        "if 'montant_total' in df.columns and 'nb_total_de_retraits' in df.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    if 'dab_hos_site' in df.columns:\n",
        "        for cat in df['dab_hos_site'].dropna().unique():\n",
        "            mask = df['dab_hos_site'] == cat\n",
        "            plt.scatter(df.loc[mask, 'nb_total_de_retraits'],\n",
        "                       df.loc[mask, 'montant_total'],\n",
        "                       alpha=0.5, label=cat)\n",
        "        plt.legend(title='Type DAB')\n",
        "    else:\n",
        "        plt.scatter(df['nb_total_de_retraits'], df['montant_total'], alpha=0.5)\n",
        "\n",
        "    plt.xlabel('Nombre de retraits')\n",
        "    plt.ylabel('Montant total (‚Ç¨)')\n",
        "    plt.title('Relation Montant - Volume')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    corr = df[['montant_total','nb_total_de_retraits']].corr().iloc[0,1]\n",
        "    print(f\"\\nüìä Corr√©lation : {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leH6n4OsYr2h"
      },
      "source": [
        "---\n",
        "# üî® SECTION 4 : Feature Engineering M√©tier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw8P0sy7Yr2h"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üî® CR√âATION DE FEATURES M√âTIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_enriched = df.copy()\n",
        "features_created = []\n",
        "\n",
        "# Feature 1: Montant moyen par retrait\n",
        "if 'montant_total' in df.columns and 'nb_total_de_retraits' in df.columns:\n",
        "    df_enriched['montant_moyen_par_retrait'] = np.where(\n",
        "        df_enriched['nb_total_de_retraits'] > 0,\n",
        "        df_enriched['montant_total'] / df_enriched['nb_total_de_retraits'],\n",
        "        np.nan\n",
        "    )\n",
        "    features_created.append('montant_moyen_par_retrait')\n",
        "    print(\"‚úÖ montant_moyen_par_retrait = montant_total / nb_retraits\")\n",
        "\n",
        "# Feature 2: Ratio vs m√©diane (montant)\n",
        "if 'montant_total' in df.columns:\n",
        "    median_mt = df_enriched['montant_total'].median()\n",
        "    df_enriched['ratio_montant_vs_median'] = df_enriched['montant_total'] / median_mt\n",
        "    features_created.append('ratio_montant_vs_median')\n",
        "    print(f\"‚úÖ ratio_montant_vs_median = montant / m√©diane ({median_mt:,.2f})\")\n",
        "\n",
        "# Feature 3: Ratio vs m√©diane (nombre)\n",
        "if 'nb_total_de_retraits' in df.columns:\n",
        "    median_nb = df_enriched['nb_total_de_retraits'].median()\n",
        "    df_enriched['ratio_nb_vs_median'] = df_enriched['nb_total_de_retraits'] / median_nb\n",
        "    features_created.append('ratio_nb_vs_median')\n",
        "    print(f\"‚úÖ ratio_nb_vs_median = nb_retraits / m√©diane ({median_nb:,.0f})\")\n",
        "\n",
        "# Feature 4: Montant attendu vs observ√©\n",
        "if 'montant_moyen_par_retrait' in df_enriched.columns:\n",
        "    median_moy = df_enriched['montant_moyen_par_retrait'].median()\n",
        "    df_enriched['montant_attendu'] = df_enriched['nb_total_de_retraits'] * median_moy\n",
        "    df_enriched['ratio_observe_vs_attendu'] = np.where(\n",
        "        df_enriched['montant_attendu'] > 0,\n",
        "        df_enriched['montant_total'] / df_enriched['montant_attendu'],\n",
        "        np.nan\n",
        "    )\n",
        "    features_created.extend(['montant_attendu', 'ratio_observe_vs_attendu'])\n",
        "    print(\"‚úÖ ratio_observe_vs_attendu = montant_observ√© / montant_attendu\")\n",
        "\n",
        "# Feature 5-6: Transformations log\n",
        "if 'montant_total' in df.columns:\n",
        "    df_enriched['log_montant'] = np.log1p(df_enriched['montant_total'])\n",
        "    features_created.append('log_montant')\n",
        "    print(\"‚úÖ log_montant = log(1 + montant)\")\n",
        "\n",
        "if 'nb_total_de_retraits' in df.columns:\n",
        "    df_enriched['log_nb_retraits'] = np.log1p(df_enriched['nb_total_de_retraits'])\n",
        "    features_created.append('log_nb_retraits')\n",
        "    print(\"‚úÖ log_nb_retraits = log(1 + nb_retraits)\")\n",
        "\n",
        "# Feature 7: Z-score\n",
        "if 'montant_total' in df.columns:\n",
        "    mean_mt = df_enriched['montant_total'].mean()\n",
        "    std_mt = df_enriched['montant_total'].std()\n",
        "    if std_mt > 0:\n",
        "        df_enriched['zscore_montant'] = (df_enriched['montant_total'] - mean_mt) / std_mt\n",
        "        features_created.append('zscore_montant')\n",
        "        print(\"‚úÖ zscore_montant = (montant - moyenne) / std\")\n",
        "\n",
        "# Feature 8-10: Indicateurs binaires\n",
        "if 'nb_total_de_retraits' in df.columns:\n",
        "    p10 = df_enriched['nb_total_de_retraits'].quantile(0.1)\n",
        "    p90 = df_enriched['nb_total_de_retraits'].quantile(0.9)\n",
        "    df_enriched['is_low_activity'] = (df_enriched['nb_total_de_retraits'] <= p10).astype(int)\n",
        "    df_enriched['is_high_activity'] = (df_enriched['nb_total_de_retraits'] >= p90).astype(int)\n",
        "    features_created.extend(['is_low_activity', 'is_high_activity'])\n",
        "    print(f\"‚úÖ is_low_activity (P10={p10:.0f}), is_high_activity (P90={p90:.0f})\")\n",
        "\n",
        "if 'dab_hos_site' in df.columns:\n",
        "    df_enriched['is_hors_site'] = (df_enriched['dab_hos_site'] == 'H').astype(int)\n",
        "    features_created.append('is_hors_site')\n",
        "    print(\"‚úÖ is_hors_site = 1 si hors site, 0 sinon\")\n",
        "\n",
        "print(f\"\\nüìä Total : {len(features_created)} features cr√©√©es\")\n",
        "\n",
        "if features_created:\n",
        "    print(\"\\nüìà Statistiques des nouvelles features :\")\n",
        "    display(df_enriched[features_created].describe(percentiles=[.05,.25,.5,.75,.95]).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDf2YwwTYr2i"
      },
      "source": [
        "---\n",
        "# ‚öñÔ∏è SECTION 5 : Pr√©paration et Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxm_ARSGYr2i"
      },
      "outputs": [],
      "source": [
        "# S√©lection des features pour la mod√©lisation\n",
        "model_features = [\n",
        "    'montant_total',\n",
        "    'nb_total_de_retraits',\n",
        "    'montant_moyen_par_retrait',\n",
        "    'ratio_montant_vs_median',\n",
        "    'ratio_nb_vs_median',\n",
        "    'ratio_observe_vs_attendu',\n",
        "    'log_montant',\n",
        "    'log_nb_retraits',\n",
        "    'zscore_montant',\n",
        "    'is_low_activity',\n",
        "    'is_high_activity',\n",
        "    'is_hors_site'\n",
        "]\n",
        "model_features = [f for f in model_features if f in df_enriched.columns]\n",
        "\n",
        "print(f\"üìã Features s√©lectionn√©es ({len(model_features)}) :\")\n",
        "for i, f in enumerate(model_features, 1):\n",
        "    print(f\"  {i:2d}. {f}\")\n",
        "\n",
        "# Dataframe de mod√©lisation\n",
        "df_model = df_enriched[model_features].copy()\n",
        "\n",
        "# Nettoyage\n",
        "df_model.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in df_model.select_dtypes(include=[np.number]).columns:\n",
        "    if df_model[col].isnull().any():\n",
        "        df_model[col].fillna(df_model[col].median(), inplace=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset pr√™t : {df_model.shape[0]:,} √ó {df_model.shape[1]}\")\n",
        "print(f\"üíæ M√©moire : {df_model.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Normalisation avec RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(df_model)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=df_model.columns, index=df_model.index)\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Normalisation effectu√©e (RobustScaler - robuste aux outliers)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE6IoYmmYr2i"
      },
      "source": [
        "---\n",
        "# ü§ñ SECTION 6 : D√©tection d'Anomalies Multi-Algorithmes\n",
        "\n",
        "## 6.1 Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78D0PVKVYr2i"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"üå≤ ISOLATION FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Param√®tres\n",
        "CONTAMINATION = 0.05  # 5% d'anomalies attendues\n",
        "\n",
        "# Mod√®le\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=500,\n",
        "    contamination=CONTAMINATION,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Entra√Ænement et pr√©diction\n",
        "predictions_if = iso_forest.fit_predict(df_scaled)\n",
        "scores_if = -iso_forest.score_samples(df_scaled)\n",
        "\n",
        "# R√©sultats\n",
        "df_enriched['if_anomaly'] = (predictions_if == -1).astype(int)\n",
        "df_enriched['if_score'] = scores_if\n",
        "\n",
        "n_anomalies_if = df_enriched['if_anomaly'].sum()\n",
        "pct_if = (n_anomalies_if / len(df_enriched) * 100).round(2)\n",
        "\n",
        "print(f\"\\n‚úÖ D√©tection termin√©e\")\n",
        "print(f\"üî¥ Anomalies : {n_anomalies_if:,} ({pct_if}%)\")\n",
        "print(f\"üü¢ Normaux   : {len(df_enriched) - n_anomalies_if:,} ({100-pct_if}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKneA_QKYr2j"
      },
      "source": [
        "## 6.2 Local Outlier Factor (LOF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUKSBG7dYr2j"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"üìç LOCAL OUTLIER FACTOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mod√®le LOF\n",
        "lof = LocalOutlierFactor(\n",
        "    n_neighbors=20,\n",
        "    contamination=CONTAMINATION,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Pr√©diction\n",
        "predictions_lof = lof.fit_predict(df_scaled)\n",
        "scores_lof = -lof.negative_outlier_factor_\n",
        "\n",
        "# R√©sultats\n",
        "df_enriched['lof_anomaly'] = (predictions_lof == -1).astype(int)\n",
        "df_enriched['lof_score'] = scores_lof\n",
        "\n",
        "n_anomalies_lof = df_enriched['lof_anomaly'].sum()\n",
        "pct_lof = (n_anomalies_lof / len(df_enriched) * 100).round(2)\n",
        "\n",
        "print(f\"\\n‚úÖ D√©tection termin√©e\")\n",
        "print(f\"üî¥ Anomalies : {n_anomalies_lof:,} ({pct_lof}%)\")\n",
        "print(f\"üü¢ Normaux   : {len(df_enriched) - n_anomalies_lof:,} ({100-pct_lof}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1z1IG3Yr2j"
      },
      "source": [
        "## 6.3 One-Class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7fbTkK2Yr2k"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"üéØ ONE-CLASS SVM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mod√®le One-Class SVM\n",
        "oc_svm = OneClassSVM(\n",
        "    nu=CONTAMINATION,  # nu ~ contamination\n",
        "    kernel='rbf',\n",
        "    gamma='auto'\n",
        ")\n",
        "\n",
        "# Pr√©diction\n",
        "predictions_svm = oc_svm.fit_predict(df_scaled)\n",
        "scores_svm = -oc_svm.score_samples(df_scaled)\n",
        "\n",
        "# R√©sultats\n",
        "df_enriched['svm_anomaly'] = (predictions_svm == -1).astype(int)\n",
        "df_enriched['svm_score'] = scores_svm\n",
        "\n",
        "n_anomalies_svm = df_enriched['svm_anomaly'].sum()\n",
        "pct_svm = (n_anomalies_svm / len(df_enriched) * 100).round(2)\n",
        "\n",
        "print(f\"\\n‚úÖ D√©tection termin√©e\")\n",
        "print(f\"üî¥ Anomalies : {n_anomalies_svm:,} ({pct_svm}%)\")\n",
        "print(f\"üü¢ Normaux   : {len(df_enriched) - n_anomalies_svm:,} ({100-pct_svm}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HKF7HgeYr2k"
      },
      "source": [
        "## 6.4 Consensus Multi-Algorithmes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2KwLJqYYr2k"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*22 + \"üéØ CONSENSUS MULTI-ALGORITHMES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Comptage du nombre d'algorithmes d√©tectant une anomalie\n",
        "df_enriched['n_algos_detecting'] = (\n",
        "    df_enriched['if_anomaly'] +\n",
        "    df_enriched['lof_anomaly'] +\n",
        "    df_enriched['svm_anomaly']\n",
        ")\n",
        "\n",
        "# Cat√©gorisation\n",
        "df_enriched['anomaly_level'] = df_enriched['n_algos_detecting'].map({\n",
        "    0: 'Normal',\n",
        "    1: 'Anomalie_Faible',\n",
        "    2: 'Anomalie_Mod√©r√©e',\n",
        "    3: 'Anomalie_Forte'\n",
        "})\n",
        "\n",
        "# Score composite (moyenne des scores normalis√©s)\n",
        "if_norm = (df_enriched['if_score'] - df_enriched['if_score'].min()) / (df_enriched['if_score'].max() - df_enriched['if_score'].min())\n",
        "lof_norm = (df_enriched['lof_score'] - df_enriched['lof_score'].min()) / (df_enriched['lof_score'].max() - df_enriched['lof_score'].min())\n",
        "svm_norm = (df_enriched['svm_score'] - df_enriched['svm_score'].min()) / (df_enriched['svm_score'].max() - df_enriched['svm_score'].min())\n",
        "\n",
        "df_enriched['composite_score'] = (if_norm + lof_norm + svm_norm) / 3\n",
        "\n",
        "# Statistiques\n",
        "print(\"\\nüìä R√©partition par niveau :\")\n",
        "level_counts = df_enriched['anomaly_level'].value_counts()\n",
        "level_pcts = (level_counts / len(df_enriched) * 100).round(2)\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'Nombre': level_counts,\n",
        "    'Pourcentage': level_pcts\n",
        "})\n",
        "display(summary)\n",
        "\n",
        "# Matrice de concordance\n",
        "print(\"\\nüìã Concordance entre algorithmes :\")\n",
        "print(f\"IF vs LOF : {((df_enriched['if_anomaly'] == df_enriched['lof_anomaly']).sum() / len(df_enriched) * 100):.1f}%\")\n",
        "print(f\"IF vs SVM : {((df_enriched['if_anomaly'] == df_enriched['svm_anomaly']).sum() / len(df_enriched) * 100):.1f}%\")\n",
        "print(f\"LOF vs SVM: {((df_enriched['lof_anomaly'] == df_enriched['svm_anomaly']).sum() / len(df_enriched) * 100):.1f}%\")\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Distribution par niveau\n",
        "level_counts.plot(kind='bar', ax=axes[0], color=['lightgreen', 'yellow', 'orange', 'red'])\n",
        "axes[0].set_title('Distribution par niveau d\\'anomalie')\n",
        "axes[0].set_ylabel('Nombre de DAB')\n",
        "axes[0].set_xlabel('')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Distribution du score composite\n",
        "for level in ['Normal', 'Anomalie_Faible', 'Anomalie_Mod√©r√©e', 'Anomalie_Forte']:\n",
        "    if level in df_enriched['anomaly_level'].values:\n",
        "        subset = df_enriched[df_enriched['anomaly_level'] == level]['composite_score']\n",
        "        axes[1].hist(subset, bins=30, alpha=0.6, label=level)\n",
        "axes[1].set_xlabel('Score composite')\n",
        "axes[1].set_ylabel('Fr√©quence')\n",
        "axes[1].set_title('Distribution du score composite')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1oPAdApYr2k"
      },
      "source": [
        "## 6.5 Visualisation PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msj4Qi1lYr2k"
      },
      "outputs": [],
      "source": [
        "# R√©duction de dimension avec PCA\n",
        "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "X_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "df_enriched['PC1'] = X_pca[:, 0]\n",
        "df_enriched['PC2'] = X_pca[:, 1]\n",
        "\n",
        "var_exp = pca.explained_variance_ratio_\n",
        "print(f\"\\nüìà Variance expliqu√©e : PC1={var_exp[0]*100:.1f}% | PC2={var_exp[1]*100:.1f}% | Total={sum(var_exp)*100:.1f}%\")\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Par niveau d'anomalie\n",
        "colors_map = {'Normal': 'green', 'Anomalie_Faible': 'yellow',\n",
        "              'Anomalie_Mod√©r√©e': 'orange', 'Anomalie_Forte': 'red'}\n",
        "for level, color in colors_map.items():\n",
        "    mask = df_enriched['anomaly_level'] == level\n",
        "    if mask.any():\n",
        "        axes[0].scatter(df_enriched.loc[mask, 'PC1'],\n",
        "                       df_enriched.loc[mask, 'PC2'],\n",
        "                       c=color, alpha=0.5, s=30, label=level)\n",
        "axes[0].set_xlabel(f'PC1 ({var_exp[0]*100:.1f}%)')\n",
        "axes[0].set_ylabel(f'PC2 ({var_exp[1]*100:.1f}%)')\n",
        "axes[0].set_title('Anomalies dans l\\'espace PCA')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot 2: Score composite\n",
        "scatter = axes[1].scatter(df_enriched['PC1'], df_enriched['PC2'],\n",
        "                         c=df_enriched['composite_score'],\n",
        "                         cmap='RdYlGn_r', alpha=0.6, s=30)\n",
        "axes[1].set_xlabel(f'PC1 ({var_exp[0]*100:.1f}%)')\n",
        "axes[1].set_ylabel(f'PC2 ({var_exp[1]*100:.1f}%)')\n",
        "axes[1].set_title('Score composite')\n",
        "plt.colorbar(scatter, ax=axes[1], label='Score')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q67yS-sUYr2k"
      },
      "source": [
        "---\n",
        "# üí° SECTION 7 : Interpr√©tabilit√© et Explications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-rItCfYr2l"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üí° G√âN√âRATION DES EXPLICATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcul des percentiles de r√©f√©rence\n",
        "percentiles = {}\n",
        "ref_cols = ['montant_moyen_par_retrait', 'montant_total', 'nb_total_de_retraits', 'ratio_observe_vs_attendu']\n",
        "ref_cols = [c for c in ref_cols if c in df_enriched.columns]\n",
        "\n",
        "for col in ref_cols:\n",
        "    percentiles[(col, 'p05')] = df_enriched[col].quantile(0.05)\n",
        "    percentiles[(col, 'p95')] = df_enriched[col].quantile(0.95)\n",
        "\n",
        "# Fonction d'explication\n",
        "def generate_explanation(row):\n",
        "    reasons = []\n",
        "\n",
        "    # Montant moyen par retrait\n",
        "    if 'montant_moyen_par_retrait' in row.index and pd.notna(row['montant_moyen_par_retrait']):\n",
        "        if row['montant_moyen_par_retrait'] < percentiles.get(('montant_moyen_par_retrait','p05'), 0):\n",
        "            reasons.append(\"Montant moyen tr√®s faible (fractionnement suspect)\")\n",
        "        elif row['montant_moyen_par_retrait'] > percentiles.get(('montant_moyen_par_retrait','p95'), float('inf')):\n",
        "            reasons.append(\"Montant moyen tr√®s √©lev√© (retraits unitaires atypiques)\")\n",
        "\n",
        "    # Volume total\n",
        "    if 'montant_total' in row.index and pd.notna(row['montant_total']):\n",
        "        if row['montant_total'] < percentiles.get(('montant_total','p05'), 0):\n",
        "            reasons.append(\"Volume total tr√®s faible\")\n",
        "        elif row['montant_total'] > percentiles.get(('montant_total','p95'), float('inf')):\n",
        "            reasons.append(\"Volume total tr√®s √©lev√©\")\n",
        "\n",
        "    # Nombre de retraits\n",
        "    if 'nb_total_de_retraits' in row.index and pd.notna(row['nb_total_de_retraits']):\n",
        "        if row['nb_total_de_retraits'] < percentiles.get(('nb_total_de_retraits','p05'), 0):\n",
        "            reasons.append(\"Tr√®s faible activit√© transactionnelle\")\n",
        "        elif row['nb_total_de_retraits'] > percentiles.get(('nb_total_de_retraits','p95'), float('inf')):\n",
        "            reasons.append(\"Activit√© transactionnelle tr√®s √©lev√©e\")\n",
        "\n",
        "    # Type de site\n",
        "    if 'dab_hos_site' in row.index and str(row['dab_hos_site']).upper() == 'H':\n",
        "        reasons.append(\"DAB hors site (profil de risque diff√©rent)\")\n",
        "\n",
        "    # Activit√©\n",
        "    if 'is_low_activity' in row.index and row['is_low_activity'] == 1:\n",
        "        reasons.append(\"DAB √† tr√®s faible activit√© (P10)\")\n",
        "    if 'is_high_activity' in row.index and row['is_high_activity'] == 1:\n",
        "        reasons.append(\"DAB √† tr√®s forte activit√© (P90)\")\n",
        "\n",
        "    return \" | \".join(reasons) if reasons else \"Profil rare multi-dimensionnel\"\n",
        "\n",
        "# G√©n√©rer explications pour les anomalies\n",
        "df_enriched['explanation'] = df_enriched.apply(\n",
        "    lambda row: generate_explanation(row) if row['anomaly_level'] != 'Normal' else '',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Explications g√©n√©r√©es pour toutes les anomalies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkdwJqK_Yr2l"
      },
      "outputs": [],
      "source": [
        "# Top anomalies avec explications\n",
        "print(\"=\"*80)\n",
        "print(\" \"*22 + \"üîù TOP 50 ANOMALIES D√âTECT√âES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "anomalies = df_enriched[df_enriched['anomaly_level'] != 'Normal'].copy()\n",
        "print(f\"\\nüìä Total anomalies : {len(anomalies):,}\")\n",
        "print(f\"   - Fortes   : {(df_enriched['anomaly_level'] == 'Anomalie_Forte').sum():,}\")\n",
        "print(f\"   - Mod√©r√©es : {(df_enriched['anomaly_level'] == 'Anomalie_Mod√©r√©e').sum():,}\")\n",
        "print(f\"   - Faibles  : {(df_enriched['anomaly_level'] == 'Anomalie_Faible').sum():,}\")\n",
        "\n",
        "if len(anomalies) > 0:\n",
        "    # Trier par score composite\n",
        "    anomalies_sorted = anomalies.sort_values('composite_score', ascending=False)\n",
        "\n",
        "    # Colonnes √† afficher\n",
        "    display_cols = [\n",
        "        'num_automate', 'lib_site_implementation', 'code_banque', 'type_carte',\n",
        "        'montant_total', 'nb_total_de_retraits', 'montant_moyen_par_retrait',\n",
        "        'dab_hos_site', 'anomaly_level', 'composite_score', 'n_algos_detecting', 'explanation'\n",
        "    ]\n",
        "    display_cols = [c for c in display_cols if c in anomalies_sorted.columns]\n",
        "\n",
        "    print(\"\\nüìã TOP 50 ANOMALIES (tri√©es par score d√©croissant) :\")\n",
        "    pd.set_option('display.max_colwidth', 100)\n",
        "    display(anomalies_sorted[display_cols].head(50))\n",
        "    pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "    # Statistiques sur les raisons\n",
        "    print(\"\\nüìä Raisons d'anomalies les plus fr√©quentes :\")\n",
        "    all_reasons = []\n",
        "    for expl in anomalies['explanation']:\n",
        "        if pd.notna(expl) and expl:\n",
        "            all_reasons.extend([r.strip() for r in expl.split('|')])\n",
        "\n",
        "    if all_reasons:\n",
        "        reason_counts = pd.Series(all_reasons).value_counts().head(10)\n",
        "        reason_pcts = (reason_counts / len(anomalies) * 100).round(2)\n",
        "\n",
        "        reason_summary = pd.DataFrame({\n",
        "            'Nombre': reason_counts,\n",
        "            'Pourcentage': reason_pcts\n",
        "        })\n",
        "        display(reason_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLfC_79Yr2l"
      },
      "source": [
        "---\n",
        "# ‚úÖ SECTION 8 : Validation Statistique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjROkxVuYr2l"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üìä VALIDATION STATISTIQUE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Comparaison distributions\n",
        "val_cols = [c for c in ['montant_total', 'nb_total_de_retraits',\n",
        "                        'montant_moyen_par_retrait', 'ratio_observe_vs_attendu']\n",
        "           if c in df_enriched.columns]\n",
        "\n",
        "if val_cols and len(anomalies) > 0:\n",
        "    normal_data = df_enriched[df_enriched['anomaly_level'] == 'Normal']\n",
        "\n",
        "    comparison = pd.DataFrame({\n",
        "        'mean_normal': normal_data[val_cols].mean(),\n",
        "        'mean_anomaly': anomalies[val_cols].mean(),\n",
        "        'median_normal': normal_data[val_cols].median(),\n",
        "        'median_anomaly': anomalies[val_cols].median(),\n",
        "        'p95_normal': normal_data[val_cols].quantile(0.95),\n",
        "        'p95_anomaly': anomalies[val_cols].quantile(0.95)\n",
        "    })\n",
        "\n",
        "    print(\"\\nüìà Comparaison Normal vs Anomalie :\")\n",
        "    display(comparison)\n",
        "\n",
        "    # Tests statistiques (Mann-Whitney U)\n",
        "    print(\"\\nüî¨ Tests statistiques (Mann-Whitney U) :\")\n",
        "    print(\"H0: Distributions identiques | H1: Distributions diff√©rentes\\n\")\n",
        "\n",
        "    test_results = []\n",
        "    for col in val_cols:\n",
        "        try:\n",
        "            statistic, p_value = stats.mannwhitneyu(\n",
        "                normal_data[col].dropna(),\n",
        "                anomalies[col].dropna(),\n",
        "                alternative='two-sided'\n",
        "            )\n",
        "            test_results.append({\n",
        "                'Variable': col,\n",
        "                'p-value': p_value,\n",
        "                'Significatif (Œ±=0.05)': 'Oui ‚úì' if p_value < 0.05 else 'Non ‚úó'\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if test_results:\n",
        "        test_df = pd.DataFrame(test_results)\n",
        "        display(test_df)\n",
        "\n",
        "        sig_count = (test_df['p-value'] < 0.05).sum()\n",
        "        print(f\"\\n‚úÖ {sig_count}/{len(test_results)} variables avec diff√©rence significative\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dzn5vjtYr2l"
      },
      "outputs": [],
      "source": [
        "# Test de stabilit√©\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*22 + \"üîÑ TEST DE STABILIT√â\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def get_top_anomalies_by_contamination(contamination, top_n=30):\n",
        "    iso_tmp = IsolationForest(\n",
        "        n_estimators=500,\n",
        "        contamination=contamination,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    pred_tmp = iso_tmp.fit_predict(df_scaled)\n",
        "    score_tmp = -iso_tmp.score_samples(df_scaled)\n",
        "\n",
        "    tmp_df = df_enriched.copy()\n",
        "    tmp_df['score_tmp'] = score_tmp\n",
        "\n",
        "    if 'num_automate' in tmp_df.columns:\n",
        "        return set(tmp_df.nlargest(top_n, 'score_tmp')['num_automate'].values)\n",
        "    else:\n",
        "        return set(tmp_df.nlargest(top_n, 'score_tmp').index)\n",
        "\n",
        "# Test avec diff√©rents niveaux\n",
        "contamination_levels = [0.03, 0.05, 0.08, 0.10]\n",
        "anomaly_sets = {}\n",
        "\n",
        "print(f\"\\nüéØ Test avec {len(contamination_levels)} niveaux de contamination\\n\")\n",
        "for cont in contamination_levels:\n",
        "    anomaly_sets[cont] = get_top_anomalies_by_contamination(cont, top_n=30)\n",
        "    print(f\"   {cont*100}% : {len(anomaly_sets[cont])} anomalies\")\n",
        "\n",
        "# Anomalies core (d√©tect√©es par tous)\n",
        "core_anomalies = set.intersection(*anomaly_sets.values())\n",
        "print(f\"\\nüéØ Anomalies CORE (d√©tect√©es √† tous les niveaux) : {len(core_anomalies)}\")\n",
        "\n",
        "# Intersection\n",
        "print(\"\\nüîó Intersections :\")\n",
        "print(f\"   3% ‚à© 5%  : {len(anomaly_sets[0.03] & anomaly_sets[0.05])}\")\n",
        "print(f\"   5% ‚à© 8%  : {len(anomaly_sets[0.05] & anomaly_sets[0.08])}\")\n",
        "print(f\"   8% ‚à© 10% : {len(anomaly_sets[0.08] & anomaly_sets[0.10])}\")\n",
        "\n",
        "print(\"\\nüí° Les anomalies CORE sont les plus robustes et critiques\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eytp_cjXYr2l"
      },
      "source": [
        "---\n",
        "# üíæ SECTION 9 : Export des R√©sultats\n",
        "\n",
        "‚ö†Ô∏è **ACTION REQUISE** : Cr√©ez d'abord un dataset vide dans Dataiku, puis modifiez `OUTPUT_DATASET_NAME`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB4ZMwQsYr2l"
      },
      "outputs": [],
      "source": [
        "# Pr√©paration du dataset d'export\n",
        "export_cols = [\n",
        "    # Identifiants\n",
        "    'num_automate', 'lib_site_implementation', 'code_banque', 'code_postale_emplacement',\n",
        "    # Variables m√©tier\n",
        "    'type_carte', 'montant_total', 'nb_total_de_retraits', 'dab_hos_site',\n",
        "    # Features engineered\n",
        "    'montant_moyen_par_retrait', 'ratio_montant_vs_median', 'ratio_observe_vs_attendu',\n",
        "    # R√©sultats d√©tection\n",
        "    'if_anomaly', 'lof_anomaly', 'svm_anomaly', 'anomaly_level', 'composite_score',\n",
        "    'if_score', 'lof_score', 'svm_score', 'n_algos_detecting',\n",
        "    # Explications\n",
        "    'explanation'\n",
        "]\n",
        "\n",
        "export_cols = [c for c in export_cols if c in df_enriched.columns]\n",
        "df_export = df_enriched[export_cols].copy()\n",
        "\n",
        "# Renommer pour clart√©\n",
        "rename_map = {\n",
        "    'if_anomaly': 'anomaly_isolation_forest',\n",
        "    'lof_anomaly': 'anomaly_lof',\n",
        "    'svm_anomaly': 'anomaly_svm',\n",
        "    'anomaly_level': 'niveau_anomalie',\n",
        "    'composite_score': 'score_composite',\n",
        "    'explanation': 'explication'\n",
        "}\n",
        "df_export.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "print(\"üì¶ Dataset d'export pr√©par√©\")\n",
        "print(f\"üìè Dimensions : {df_export.shape[0]:,} √ó {df_export.shape[1]}\")\n",
        "print(f\"\\nüìã Colonnes export√©es ({len(df_export.columns)}) :\")\n",
        "for i, col in enumerate(df_export.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4kI3bv_Yr2m"
      },
      "outputs": [],
      "source": [
        "# Export vers Dataiku\n",
        "OUTPUT_DATASET_NAME = \"DAB_ANOMALIES_RESULTATS\"  # ‚ö†Ô∏è MODIFIER ICI\n",
        "\n",
        "try:\n",
        "    output_dataset = dataiku.Dataset(OUTPUT_DATASET_NAME)\n",
        "    output_dataset.write_with_schema(df_export)\n",
        "\n",
        "    print(f\"\\n‚úÖ Export r√©ussi vers '{OUTPUT_DATASET_NAME}'\")\n",
        "    print(f\"üìä {len(df_export):,} lignes export√©es\")\n",
        "    print(f\"\\nüìà Statistiques :\")\n",
        "    print(f\"   - Anomalies fortes   : {(df_export['niveau_anomalie'] == 'Anomalie_Forte').sum():,}\")\n",
        "    print(f\"   - Anomalies mod√©r√©es : {(df_export['niveau_anomalie'] == 'Anomalie_Mod√©r√©e').sum():,}\")\n",
        "    print(f\"   - Anomalies faibles  : {(df_export['niveau_anomalie'] == 'Anomalie_Faible').sum():,}\")\n",
        "    print(f\"   - Normaux            : {(df_export['niveau_anomalie'] == 'Normal').sum():,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erreur d'export : {e}\")\n",
        "    print(\"\\nüí° Solutions :\")\n",
        "    print(\"   1. Cr√©ez le dataset dans Dataiku\")\n",
        "    print(\"   2. V√©rifiez le nom du dataset\")\n",
        "    print(\"   3. V√©rifiez vos permissions\")\n",
        "    print(\"\\nüì¶ Les r√©sultats restent disponibles dans df_export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yisux6CYr2m"
      },
      "source": [
        "---\n",
        "# üìä SECTION 10 : R√©sum√© Ex√©cutif et Recommandations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNIOU9eeYr2m"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \"*15 + \"üìä R√âSUM√â EX√âCUTIF - D√âTECTION D'ANOMALIES DAB\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Volum√©trie\n",
        "print(\"\\n1Ô∏è‚É£ VOLUM√âTRIE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total DAB analys√©s           : {len(df):,}\")\n",
        "print(f\"Features utilis√©es           : {len(model_features)}\")\n",
        "print(f\"Algorithmes de d√©tection     : 3 (IF + LOF + One-Class SVM)\")\n",
        "\n",
        "# 2. R√©sultats\n",
        "print(\"\\n2Ô∏è‚É£ R√âSULTATS DE D√âTECTION\")\n",
        "print(\"=\"*60)\n",
        "n_forte = (df_enriched['anomaly_level'] == 'Anomalie_Forte').sum()\n",
        "n_moderee = (df_enriched['anomaly_level'] == 'Anomalie_Mod√©r√©e').sum()\n",
        "n_faible = (df_enriched['anomaly_level'] == 'Anomalie_Faible').sum()\n",
        "n_normal = (df_enriched['anomaly_level'] == 'Normal').sum()\n",
        "\n",
        "print(f\"üî¥ Anomalies FORTES (3 algos)   : {n_forte:,} ({n_forte/len(df)*100:.2f}%)\")\n",
        "print(f\"üü† Anomalies MOD√âR√âES (2 algos) : {n_moderee:,} ({n_moderee/len(df)*100:.2f}%)\")\n",
        "print(f\"üü° Anomalies FAIBLES (1 algo)   : {n_faible:,} ({n_faible/len(df)*100:.2f}%)\")\n",
        "print(f\"üü¢ DAB NORMAUX                  : {n_normal:,} ({n_normal/len(df)*100:.2f}%)\")\n",
        "\n",
        "# 3. Principaux patterns\n",
        "print(\"\\n3Ô∏è‚É£ PATTERNS D'ANOMALIES IDENTIFI√âS\")\n",
        "print(\"=\"*60)\n",
        "if 'all_reasons' in locals() and all_reasons:\n",
        "    top_reasons = pd.Series(all_reasons).value_counts().head(5)\n",
        "    for i, (reason, count) in enumerate(top_reasons.items(), 1):\n",
        "        pct = (count / len(anomalies) * 100).round(1)\n",
        "        print(f\"{i}. {reason}\")\n",
        "        print(f\"   ‚Üí {count:,} DAB ({pct}% des anomalies)\\n\")\n",
        "\n",
        "# 4. Stabilit√©\n",
        "print(\"4Ô∏è‚É£ ROBUSTESSE DES D√âTECTIONS\")\n",
        "print(\"=\"*60)\n",
        "if 'core_anomalies' in locals():\n",
        "    print(f\"Anomalies CORE (stables) : {len(core_anomalies)} DAB\")\n",
        "    print(f\"‚Üí D√©tect√©es peu importe le param√©trage (haute confiance)\")\n",
        "\n",
        "# 5. Recommandations\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5Ô∏è‚É£ RECOMMANDATIONS OP√âRATIONNELLES\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "üéØ PRIORISATION DES ACTIONS :\n",
        "\n",
        "PRIORIT√â 1 - Anomalies FORTES (consensus 3 algorithmes)\n",
        "   ‚îú‚îÄ Investigation imm√©diate obligatoire\n",
        "   ‚îú‚îÄ V√©rification logs syst√®me et historique\n",
        "   ‚îú‚îÄ Audit de s√©curit√© si suspicion fraude\n",
        "   ‚îî‚îÄ Blocage pr√©ventif si risque √©lev√©\n",
        "\n",
        "PRIORIT√â 2 - Anomalies CORE (stables)\n",
        "   ‚îú‚îÄ Anomalies persistantes = action requise\n",
        "   ‚îú‚îÄ Analyse des causes racines\n",
        "   ‚îî‚îÄ Plan d'action correctif\n",
        "\n",
        "PRIORIT√â 3 - Anomalies MOD√âR√âES (2 algorithmes)\n",
        "   ‚îú‚îÄ Surveillance renforc√©e\n",
        "   ‚îú‚îÄ Investigation si r√©currence\n",
        "   ‚îî‚îÄ Cat√©gorisation m√©tier\n",
        "\n",
        "PRIORIT√â 4 - Anomalies FAIBLES (1 algorithme)\n",
        "   ‚îú‚îÄ Monitoring passif\n",
        "   ‚îî‚îÄ Escalade si √©volution vers niveau sup√©rieur\n",
        "\n",
        "üîÑ ACTIONS DE SUIVI :\n",
        "\n",
        "1. Monitoring continu\n",
        "   ‚îî‚îÄ R√©-ex√©cution hebdomadaire/mensuelle de l'analyse\n",
        "\n",
        "2. Enrichissement des donn√©es\n",
        "   ‚îú‚îÄ Ajouter donn√©es temporelles (jour/heure/saison)\n",
        "   ‚îú‚îÄ Int√©grer g√©olocalisation enrichie\n",
        "   ‚îú‚îÄ Historiser pour d√©tection de tendances\n",
        "   ‚îî‚îÄ Ajouter donn√©es externes (m√©t√©o, √©v√©nements)\n",
        "\n",
        "3. Feedback m√©tier\n",
        "   ‚îú‚îÄ Valider anomalies avec √©quipes terrain\n",
        "   ‚îú‚îÄ Documenter faux positifs\n",
        "   ‚îú‚îÄ Ajuster seuils si n√©cessaire\n",
        "   ‚îî‚îÄ Cr√©er taxonomie des anomalies\n",
        "\n",
        "4. Am√©lioration continue\n",
        "   ‚îú‚îÄ Tester algorithmes suppl√©mentaires (DBSCAN, Autoencoder)\n",
        "   ‚îú‚îÄ Feature engineering avanc√© (r√©seaux, saisonnalit√©)\n",
        "   ‚îú‚îÄ Syst√®me d'alerting automatique\n",
        "   ‚îî‚îÄ Dashboard op√©rationnel temps r√©el\n",
        "\n",
        "üí° INDICATEURS CL√âS √Ä SURVEILLER :\n",
        "\n",
        "- Taux de confirmation des anomalies par les √©quipes m√©tier\n",
        "- Temps moyen de r√©solution par niveau d'anomalie\n",
        "- Impact financier des anomalies d√©tect√©es\n",
        "- √âvolution du nombre d'anomalies dans le temps\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"‚úÖ ANALYSE TERMIN√âE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìÖ Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üíæ R√©sultats : df_export ({len(df_export):,} lignes)\")\n",
        "print(f\"üìÅ Dataset Dataiku : {OUTPUT_DATASET_NAME}\")\n",
        "print(\"\\nüéâ Analyse compl√®te et valid√©e !\")\n",
        "print(\"\\nüí° Prochaines √©tapes sugg√©r√©es :\")\n",
        "print(\"   1. Examiner les top anomalies avec les √©quipes m√©tier\")\n",
        "print(\"   2. Valider les explications g√©n√©r√©es\")\n",
        "print(\"   3. Cr√©er un plan d'action par niveau d'anomalie\")\n",
        "print(\"   4. Programmer l'ex√©cution automatique hebdomadaire\")\n",
        "print(\"   5. Cr√©er un dashboard de suivi des anomalies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u47xbHKUYr2m"
      },
      "source": [
        "---\n",
        "\n",
        "# üìù Notes Techniques\n",
        "\n",
        "## M√©thodologie\n",
        "\n",
        "Cette analyse impl√©mente une approche **multi-algorithmes robuste** :\n",
        "\n",
        "### ‚úÖ Points Forts\n",
        "\n",
        "1. **Validation crois√©e** : 3 algorithmes compl√©mentaires (IF + LOF + One-Class SVM)\n",
        "2. **Feature engineering** : 10+ features m√©tier cr√©√©es\n",
        "3. **Normalisation robuste** : RobustScaler r√©sistant aux outliers\n",
        "4. **Tests de stabilit√©** : V√©rification robustesse des d√©tections\n",
        "5. **Validation statistique** : Tests Mann-Whitney U\n",
        "6. **Interpr√©tabilit√©** : Explications automatiques par anomalie\n",
        "\n",
        "### üîß Param√®tres Ajustables\n",
        "\n",
        "- `CONTAMINATION` : Taux d'anomalies attendu (d√©faut 5%)\n",
        "- `N_NEIGHBORS` (LOF) : Nombre de voisins (d√©faut 20)\n",
        "- `nu` (One-Class SVM) : Borne sup√©rieure fraction outliers\n",
        "- Features s√©lectionn√©es : Adapter selon donn√©es disponibles\n",
        "\n",
        "### üìö Pour Aller Plus Loin\n",
        "\n",
        "**Am√©liorations possibles** :\n",
        "\n",
        "1. **Analyse temporelle** : Int√©grer s√©ries temporelles et saisonnalit√©\n",
        "2. **Clustering** : Segmenter DAB avant d√©tection (urbain/rural)\n",
        "3. **Deep Learning** : Autoencoders pour patterns complexes\n",
        "4. **Graphes** : Analyser r√©seaux de DAB g√©ographiques\n",
        "5. **Ensemble stacking** : Combiner pr√©dictions de mani√®re optimale\n",
        "\n",
        "---\n",
        "\n",
        "**Auteur** : Data Science Team  \n",
        "**Version** : 2.0  \n",
        "**Contact** : Pour questions ou am√©liorations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}